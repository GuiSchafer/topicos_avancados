{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 22:14:21.057530: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-24 22:14:21.174712: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-24 22:14:21.174738: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-24 22:14:21.196003: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-24 22:14:21.943730: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-24 22:14:21.943921: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-24 22:14:21.943936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_feather('./tavbase/GS.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meia boca \n",
    "def deep_learning_dnn(df_dl, dep_var, classes):\n",
    "    # Separa a variável dependente das demais\n",
    "    deep_feat = df_dl.drop(columns=[dep_var], axis=1)\n",
    "    deep_label = df_dl[dep_var]\n",
    "    # Verifica os tipos das variáveis\n",
    "    categorical_columns = [col for col in deep_feat.columns if len(deep_feat[col].unique()) == 2 or deep_feat[col].dtype == 'O']\n",
    "    continuous_columns = [col for col in deep_feat.columns if len(deep_feat[col].unique()) > 2 and (deep_feat[col].dtype == 'int64' or deep_feat[col].dtype == 'float64')]\n",
    "    # Verifica as colunas para normalização - as demais serão discretizadas - Função Bucketize do Tensor Flow\n",
    "    cols_to_scale = continuous_columns[:]\n",
    "    #cols_to_scale.remove('meses')\n",
    "    # Ajusta as bases de treino e de teste\n",
    "    XX_T = df_dl.drop(columns=[dep_var], axis=1)\n",
    "    XX_t = df_dl.drop(columns=[dep_var], axis=1)\n",
    "    yy_T = df_dl[dep_var]\n",
    "    yy_t = df_dl[dep_var]\n",
    "    # Normaliza as variáveis nas bases de treino e teste\n",
    "    scaler = StandardScaler()\n",
    "    XX_T.loc[:, cols_to_scale] = scaler.fit_transform(XX_T.loc[:, cols_to_scale])\n",
    "    XX_t.loc[:, cols_to_scale] = scaler.fit_transform(XX_t.loc[:, cols_to_scale])\n",
    "    # Ajustes das Variáveis Categórica - Não presentes neste modelo\n",
    "    categorical_object_feat_cols = [tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_hash_bucket(key=col, hash_bucket_size=1000), dimension=len(df_dl[col].unique()))\n",
    "    for col in categorical_columns if df_dl[col].dtype == 'O']\n",
    "    # Ajustes das Variáveis Categórica - Não presentes neste modelo\n",
    "    categorical_integer_feat_cols = [tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_identity(key=col, num_buckets=2), dimension=len(df_dl[col].unique()))\n",
    "    for col in categorical_columns if df[col].dtype=='int64']\n",
    "    continuous_feat_cols = [tf.feature_column.numeric_column(key=col) for col in continuous_columns] \n",
    "    feat_cols = categorical_object_feat_cols + \\\n",
    "                categorical_integer_feat_cols + \\\n",
    "                continuous_feat_cols\n",
    "    # Rotina de DNN (Deep Neural Network)\n",
    "    input_fun = tf.compat.v1.estimator.inputs.pandas_input_fn(XX_T, yy_T, batch_size=50, num_epochs=1000, shuffle=True)\n",
    "    pred_input_fun = tf.compat.v1.estimator.inputs.pandas_input_fn(XX_t, batch_size=50, shuffle=False)\n",
    "    DNN_model = tf.estimator.DNNClassifier(hidden_units=[10, 10, 10], feature_columns=feat_cols, n_classes=classes)\n",
    "    DNN_model.train(input_fn=input_fun, steps=5000)\n",
    "    # Resgata os resultados da DNN\n",
    "    predictions = DNN_model.predict(pred_input_fun)\n",
    "    pred = list(predictions)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grupo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/topicos_avancados/Aula14.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://guischafer-topicosavanc-gewrupmlmfn.ws-us72.gitpod.io/workspace/topicos_avancados/Aula14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m probabilidade \u001b[39m=\u001b[39m deep_learning_dnn(grupo, \u001b[39m'\u001b[39m\u001b[39mBenefit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://guischafer-topicosavanc-gewrupmlmfn.ws-us72.gitpod.io/workspace/topicos_avancados/Aula14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m probabilidade_classe \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://guischafer-topicosavanc-gewrupmlmfn.ws-us72.gitpod.io/workspace/topicos_avancados/Aula14.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(probabilidade)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grupo' is not defined"
     ]
    }
   ],
   "source": [
    "probabilidade = deep_learning_dnn(grupo, 'Benefit', 2)\n",
    "probabilidade_classe = []\n",
    "for i in range(len(probabilidade)):\n",
    "    probabilidade_classe.append(probabilidade[i][\"class_ids\"][0])\n",
    "probabilidade_prob0 = []\n",
    "for i in range(len(probabilidade)):\n",
    "    probabilidade_prob0.append(probabilidade[i][\"probabilities\"][0])\n",
    "probabilidade_prob1 = []\n",
    "for i in range(len(probabilidade)):\n",
    "    probabilidade_prob1.append(probabilidade[i][\"probabilities\"][1]) \n",
    "grupo['dl_classe'] = probabilidade_classe\n",
    "grupo['lucro_0'] = probabilidade_prob0\n",
    "grupo['lucro_1'] = probabilidade_prob1\n",
    "grupo = grupo.reset_index()\n",
    "grupo\n",
    "\n",
    "grupo.to_feather('probabilidade_pais.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
